{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import ctypes\n",
    "   \n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "class ObjectDetector(object):\n",
    "   \n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "       \n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "   \n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n",
    "\n",
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "   \n",
    "    #this changing of this value will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "   \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "       \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "       \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "       \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()  \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        #start capture the first depth image\n",
    "        depth_frame = frames.get_depth_frame()          \n",
    "        self.depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap  \n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "           \n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable\n",
    "\n",
    "            depth_frame = frames.get_depth_frame() #get the depth image          \n",
    "            self.depth_image = np.asanyarray(depth_frame.get_data()) #convert depth data to numpy array\n",
    "            #conver depth data to BGR image for displaying purpose\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            self.depth_value = depth_colormap #assign the color BGR image to the depth value\n",
    "   \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread      \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PROCESS\n",
    "\n",
    "# if first frame\n",
    "#   if there are detections\n",
    "#       find nearest person\n",
    "#       find coordinates of the center of their bounding box\n",
    "#       move accordingly towards the boudning box\n",
    "#\n",
    "#   else: (no detections)\n",
    "#       sleep for a second\n",
    "#\n",
    "# else: (not first frame)\n",
    "#   if there are detections\n",
    "#       if any detection is at least 0.65 (correlation) similar to the person being followed in previous frame\n",
    "#           choose the first detection that satisfies this\n",
    "#           find the center of their bounding box\n",
    "#           move accordingly towards the boudning box\n",
    "#\n",
    "#       else: (no detections are at least 0.65 similar to the person from previous frame)\n",
    "#           choose the nearest detection [0]\n",
    "#           find the center of their bounding box\n",
    "#           move accordingly towards the bounding box\n",
    "#\n",
    "#   else: (no detections)\n",
    "#       sleep for a second\n",
    "\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# cent = (width, height)\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget,]),\n",
    "    label_widget\n",
    "]))\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = Robot()\n",
    "runs = 0 # keeps track of frame\n",
    "\n",
    "def crop_bb(image, bbox):\n",
    "    full_coords = [width*bbox[0], height*bbox[1], width*bbox[2], height*bbox[3]]\n",
    "    full_coords = [round(_) for _ in full_coords]\n",
    "\n",
    "    cropped_person = image[full_coords[1]:full_coords[3], full_coords[0]:full_coords[2]]\n",
    "\n",
    "    return cropped_person\n",
    "\n",
    "def calc_hist(image):\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0,255])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "def processing(change):\n",
    "    global runs\n",
    "    image = change['new']\n",
    "     \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "   \n",
    "    print(detections[0])\n",
    "    if runs == 0:\n",
    "        # checks to see if this is the first frame\n",
    "        matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "        distance_human = 0\n",
    "        # finds all the detections in the frame\n",
    "        if matching_detections:\n",
    "            # if any person has been detected, ...\n",
    "            runs += 1\n",
    "            first_person = matching_detections[0]\n",
    "            # gets the closest detected instance to the camera\n",
    "            bbox = first_person['bbox']\n",
    "            cent =[(int(width * bbox[0])+int(width * bbox[2]))//2,(int(height * bbox[1])+int(height * bbox[3]))//2]\n",
    "            # finds the center of the bounding box of an instance\n",
    "            distance_human = camera.depth_image[cent[0],cent[1]]\n",
    "            # calculates instance distance from robot\n",
    "            cv2.putText(image, str(distance_human), (cent[0], cent[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            movement(cent, distance_human)\n",
    "\n",
    "            cropped_boundingbox = crop_bb(image, bbox)\n",
    "            old_histogram = calc_hist(cropped_boundingbox)\n",
    "        else:\n",
    "            # if no detections are found, the robot waits\n",
    "            time.sleep(1.0)\n",
    "    else:\n",
    "        # if it is not on the first frame\n",
    "        matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "        # finds all detected isntances\n",
    "        if matching_detections:\n",
    "            # if at least one person has been detected\n",
    "            person_index = same_person(matching_detections, old_histogram)\n",
    "            # finds the index (in matching_detections) of the first detection that is considered similar to the person being followed\n",
    "            person_bb = matching_detections[person_index]\n",
    "            cent = [(int(width * person_bb[0])+int(width * person_bb[2]))//2,(int(height * person_bb[1])+int(height * person_bb[3]))//2]\n",
    "            # finds center of bounding box\n",
    "            distance_human = camera.depth_image[cent[0],cent[1]]\n",
    "            # finds distance of instance from robot\n",
    "            cv2.putText(image, str(distance_human), (cent[0], cent[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            movement(cent, distance_human)\n",
    "            crop_person = crop_bb(image, person_bb)\n",
    "            old_histogram = calc_hist(crop_person)\n",
    "        else:\n",
    "            time.sleep(1.0)\n",
    "    image_widget.value = bgr8_to_jpeg(image)      \n",
    "\n",
    "\n",
    "\n",
    "def same_person(matching_detections, old_histogram, image):\n",
    "    \"\"\" \n",
    "        Iterates through all detections to see if any of the detections are similar enough to the perosn being followed from the last frame to be considered the same\n",
    "\n",
    "        matching_detections - a dictionary containing information on every detected instance\n",
    "        old_histogram - the normalised image histogram of the person in the previous frame\n",
    "        image - the frame to crop bounding boxes from\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx, i in enumerate(matching_detections):\n",
    "        bbox = i['bbox']\n",
    "        cropped_person = crop_bb(image, bbox)\n",
    "        # crops the instances bounding box from the frame\n",
    "    \n",
    "        new_histogram = calc_hist(cropped_person)\n",
    "        # calculates the image histogram of the cropped boounding box\n",
    "        \n",
    "        if cv2.compareHist(new_histogram, old_histogram, cv2.HISTCMP_CORREL) > 0.65:\n",
    "            # checks to see if each bounding box's histogram is similar enough to the person instance to be considered the same instance\n",
    "            return i\n",
    "    return 0\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "def movement(cent, distance_human):\n",
    "    \"\"\"\n",
    "        Controls how the robot will move, taking into consideration how off centre the target instance is and how far away it is\n",
    "        \n",
    "        cent - center of the instance (to be followed) bounding box\n",
    "        distance_human - distance of instance from robot\n",
    "    \"\"\"\n",
    "    threshold = 20\n",
    "    distance = width // 2 - cent[0] # distance from center line to center of bbox\n",
    "    prev_dir = 'n/a'\n",
    "    base_speed = 0.3 # Default speed\n",
    "    if distance_human >1000:\n",
    "        prev_dir = 'forward'\n",
    "        robot.forward(0.3)\n",
    "    elif distance > threshold:\n",
    "        # turn left\n",
    "        speed = base_speed * distance # calculates movements speed dependent on position of target \n",
    "        robot.left(speed)\n",
    "        prev_dir = 'left'\n",
    "    elif distance < -threshold:\n",
    "        speed = base_speed * -distance # calculates movements speed dependent on position of target \n",
    "        #  turn right\n",
    "        robot.right(speed)\n",
    "        prev_dir = 'right'\n",
    "    else:\n",
    "        if prev_dir == 'forward':\n",
    "            robot.forward(0.3)\n",
    "        elif prev_dir == 'left':\n",
    "            robot.left(0.2)\n",
    "        elif prev_dir == 'right':\n",
    "            robot.right(0.2)\n",
    "        else:\n",
    "            robot.stop()\n",
    "        \n",
    "\n",
    "   \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#  for det in matching_detections:\n",
    "# #         bbox = det['bbox']\n",
    "# #         cent =[(int(width * bbox[0])+int(width * bbox[2]))//2,(int(height * bbox[1])+int(height * bbox[3]))//2]\n",
    "# #         cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "\n",
    "# #         if cent[0] < height and cent[1] < width:\n",
    "       \n",
    "# #             distance_human = camera.depth_image[cent[0],cent[1]]\n",
    "# #             cv2.putText(image, str(distance_human), (cent[0], cent[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# #             width_bbox0 = int(width * bbox[0])\n",
    "# #             width_bbox2 = int(width * bbox[2])\n",
    "# #             height_bbox1 = int(height * bbox[1])\n",
    "# #             height_bbox2 = int(height * bbox[3])\n",
    "\n",
    "# #             print('widthb0 =', width_bbox0)\n",
    "# #             print('wb2 =', width_bbox2)\n",
    "# #             print('hb1=', height_bbox1)\n",
    "# #             print('hb2=', height_bbox2)\n",
    "       \n",
    "       \n",
    "#     if len(matching_detections)==0:\n",
    "#         robot.stop()\n",
    "#     else:\n",
    "#         if distance_human >1000:\n",
    "#             robot.forward(0.5)\n",
    "#         elif distance_human >400:\n",
    "#             robot.forward(0.3)\n",
    "#         else:\n",
    "#             robot.stop()\n",
    "       \n",
    "#     image_widget.value = bgr8_to_jpeg(image)\n",
    "   \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "# camera.observe(processing, names='color_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "camera.unobserve_all()\n",
    "camera.stop()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
